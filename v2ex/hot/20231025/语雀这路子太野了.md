### 语雀这路子太野了

https://mp.weixin.qq.com/s/WFLLU8R4bmiqv6OGa-QMcw
他们的公告的链接
考虑到 v 友的水平，我抛砖引玉分析一下
这帖子的意思大概是说，由于临时工在升级维护工具的时候，工具没有严格测试，直接上生产环境，工具的 bug 导致数据库服务器下线，联系硬件团队，硬件团队说上不了线，摆烂不玩了，你们自己恢复备份吧，然后花了四个小时恢复，俩小时验证数据，成功上线
我和几个朋友讨论了下，觉得非常的，不可思议
这是 2023 年的，语雀这个体量的公司，做出来的事情
正常的架构思维里，所有的服务，就不应该跑在同一台机器上，包括数据库，最次也该是个主从集群，集群下面的机器单例再考虑 raid 之类的东西
在这个设计下，不存在上不了线开不了机这种事情，机房被修卡军团占领了都没事
至于网上传的什么之前的技术负责人跑路了，新人不会操作
就正常的 devops ，后台管理面板里，全自动维护，包括版本控制，回滚，备份，集群，镜像，机器冗余，全部自动化管理
这不该是现在的标配吗
技术负责人跑路，新人不会操作，这句话假定的前提是，这一切都是手工完成的
语雀这么大公司，表现得跟路边三五个人创业的草台班子一样

---------------------------------------------------

10 月 23 日下午，服务语雀的数据存储运维团队在进行升级操作

这么大的公司竟然选择下午升级，语雀正常用户大部分都是程序员吧，这些人一般上午开开早会，然后下午干活，正好用户使用高峰期，这个升级时间选择，就 tm 的离谱，

---------------------------------------------------

别侮辱路边三五个人创业的草台班子，他们做的会更好。

---------------------------------------------------

人类组织的本质都是草台班子，那些看起来不草台的无非是里面有一小部分很牛逼的人顶着没出问题

---------------------------------------------------

很正常啊,支付宝当年不也一铲子下去之后就全国都挂了.所以那些扯什么异地双活啊异地灾备啊听听就算了.

---------------------------------------------------

阿里现在真是太拉了，每年都能出个 P0 的大故障，阿里云香港机房 C 区全面故障还记得吧。

---------------------------------------------------

听说外包团队已经开了，不知道真假

---------------------------------------------------

盲猜是很久之前上线的服务用了阿里云的旧型号的 ECS ，不小心删掉了，就没办法再买一个原样的出来了
然后一堆服务在新型号的机器上不兼容，只能手工处理

---------------------------------------------------

@cherbim 正常更新时间应该是周四，这是阿里标配，我在 b 站看极海 Channel 说的，所以这个是典型的，没有走版本控制，代码审计，连自动化测试都没有，“临时工“闭着眼睛直接上生产环境的案例

---------------------------------------------------

很明显，是裁员了，新接手的大学生不熟练

---------------------------------------------------

现在阿里系的服务吹什么高可用，都可以打个大大的问号了

---------------------------------------------------

所以，所谓的下线是指把云服务器的实例给删除了嘛？

---------------------------------------------------

@yhxx 你这个分析太离谱了，首先，跑在容器里的服务，绕过阿里云的账户权限管理，把容器给删了，这个事情是做不到的，那就只能是拥有后台权限的人自己操作，删库跑路了

---------------------------------------------------

带头人走了，团队变得松散，重要但万年不用一次的关键环节被忽视，不奇怪。当初把大部分人员抽走搞钉钉文档，结果钉钉文档没搞起来，语雀重新扩充团队却越搞越好，其实从这点就可以看出项目带头人的重要性。

以阿里的尿性，不能自负盈亏的项目都搞不长久，不久的将来，阿里旗下几大文档必定砍掉部分，不知道会不会是语雀。

---------------------------------------------------

> 14:15 联系硬件团队尝试将下线机器重新上线； 15:00 确认因存储系统使用的机器类别较老，无法直接操作上线，立即调整恢复方案为从备份系统中恢复存储数据。

被你说成

> 联系硬件团队，硬件团队说上不了线，摆烂不玩了，你们自己恢复备份吧

造谣真简单啊

---------------------------------------------------

你的思考里面，从 跑在同一台机器上 这里开始就已经是全错了

---------------------------------------------------

看起来是存储的集群太老了，有一些隐患没人敢优化，毕竟这种优化可能会造成无功有过。于是击鼓传花，直到一个倒霉蛋引爆了它。


不过我疑问的是，使用冷备恢复的数据，公告里还说所有数据都没丢失是怎么做到的。备份之后到问题发生之前的那些数据呢？

---------------------------------------------------

@yyzh 有一说一，这个事情之后的多年直到今天，支付宝再没挂过。语雀看起来并没有用支付宝那套主流部署架构。

---------------------------------------------------

openai 不也照样经常 down ，有时也几个小时

---------------------------------------------------

质疑草台班子 理解草台班子 成为草台班子

---------------------------------------------------

运维工具，是不是引入了 terraform 之类的自动化定义，然后把资源给删除了？

---------------------------------------------------

@pengtikui 机器太老了，上不了线，这个事情本身就是有点不可思议的，你看前几楼的回复，他们都认为是跑在阿里的 ECS 上，而不是自建机房才能通过删除 ECS 实例达成上不了线这个结果，但是看他们公告，他们的说法明显是自建机房，可能我技术很菜吧，我理解不了他这句话是怎么做到的，机器都在自己手里，是下线了又不是删库了，那我只能理解成摆烂不玩了

---------------------------------------------------

反正也越来越不好用了，早就不用很久了

---------------------------------------------------

这个从备份中恢复我也很好奇

从备份恢复没有丢失数据, 怎么做到的

类似快照之类的应该都有个备份点, 不太可能不丢失数据 


除非还依赖实时备份的数据库日志, 从日志中恢复增量部分

---------------------------------------------------

@isbase 以前那是支付宝内部 P0 故障，现在支付宝微信这种体量的再出这种直接工信部 P0 故障，你掂量掂量🤪

---------------------------------------------------

7 小时完成离线新建存储系统，至少是公司内的最快记录了吧

---------------------------------------------------

？假面骑士 1 号

---------------------------------------------------

@Mess1ah 别光喷呀，有何高见

---------------------------------------------------

和最近 cs2 更新一样
各种奇奇怪怪的 bug 满天飞
修好这个又起来那个

---------------------------------------------------

直接原因是外包或者新手写了有问题的运维工具。和当年携程一模一样。
反正运维在 IT 生态连最底端，根上就没治。

---------------------------------------------------

存储节点怎么会跑在容器里呢？这种服务的存储大概率是用物理机来扛的，虚拟机性能撑不住。所以才有联系硬件团队下线重新上线的流程。
硬件团队不敢动，大概率是这机器用了很久了，和现有大环境的机型都不一样，当初为了做网络储存打通下了很多非标配置，可能没有持久化。这种史前机器，后续接手的人根本没办法短时间恢复，所以建议直接从备份恢复到新机器上一劳永逸。

---------------------------------------------------

来猜下这个过程是什么样的

估计是运维工具 bug 导致华东 region 的一批机器下线，这批机型要么很老那么已经过保还未更换，关机再启动很多服务拉不起来，数据库又没有跨 region 部署，只能重新把本地盘或者云盘挂载到能用的机型或者 ecs 上做数据恢复，要是这样的话，耗时确实会很久

---------------------------------------------------

甩锅外包

---------------------------------------------------

@pkoukk 是容器还是直接在物理机上，在这个问题上，我认为是无关紧要的，而且这种体量，正常的思维必然是上集群来扩容，不管是加物理机器还是加容器都一样，数据库分布式的存储在不同的机器上，查询的时候也有很多便利的算法可以用，再给每个节点上个从属服务器，自动同步，在这个设计下，机器是新的是旧的，几台机器炸了，或者一两个机房被修卡军团占领了，都无关紧要的

---------------------------------------------------

所有的团队都是草台班子

---------------------------------------------------

公告就是安抚用户的，你还真信里面的内容啊，太天真了吧。

---------------------------------------------------

@pengtdyd 别光喷呀，有何高见

---------------------------------------------------

其实我还是没有很搞懂，为啥运维工具可以直接下线服务器，理论上下线生产服务器这种操作，需要二次的吧

---------------------------------------------------

@minami 卧槽 表达真到位！

---------------------------------------------------

@mazyi 一种我目测最合理的可能，运维工具把那台机器的环境搞炸了，机器又是祖传的，根本恢复不动，服务跑不起来，所以上不了线，备份也不是真的备份，就是原始数据，他们新搭了个机器，在全新的，干净的环境里，起了服务，导入了数据

---------------------------------------------------

故障恢复文档放在语雀了，死锁了。

---------------------------------------------------

@cherbim 白天升级比晚上好。晚上人的状态不好，而且经常人数工种不齐全

---------------------------------------------------

另外，对于寄希望于“点点面板就可以”，我只能说这种大众认知其实正是事故隐患的来源

---------------------------------------------------

有效信息太少，没法评价

---------------------------------------------------

@mazyi 具体情况不知道。不过你问的这种情况确实有可能发生：

比如语雀作为赖着不走的用户占用一个虚拟机，这个虚拟机的宿主是一个旧硬件，已经被阿里云列入淘汰计划
这次语雀用户升级时，在没注意到里面有些“本地存储数据”的情况下，释放了这个虚拟机，然后其宿主机直接被自动化踢出，甚至格式化了

这种情况，即使之前做过演练也不一定能发现，因为演练的时候不一定会伴随发生 IaaS 层的变化，虚拟机里的本地存储数据在演练时也不一定重要到能看出事故的程度

---------------------------------------------------

@julyclyde 这其实是我的经验之谈，你不信任精心设计和经过反复测试和验证的自动化处理，却相信凭经验和感觉的人工手动操作吗

---------------------------------------------------

文档服务，你搞数据库主从集群，你这路子更野。

---------------------------------------------------

@nekoharuya 你这样说其实是假设面板都经过反复测试验证了
这种事能随便假设么

---------------------------------------------------

@nothingistrue 别光喷呀，有何高见

---------------------------------------------------

应该是当初设计架构的可能都走了，这块从来没做过，没交接下来

---------------------------------------------------

@julyclyde 小公司自己手动操作确实居多，我以前的时候，也是从手动操作->自动脚本->gui 工具，这样积累起来，但是，语雀这么大个公司

---------------------------------------------------

@nekoharuya 其实大公司也有大公司的问题
提倡使用面板，看似是减少误操作，其实也减少了人员的训练
把“工程师”降成了“操作员”
一旦出现问题，很可能会发现他们连紧急抢救的技能都忘了

---------------------------------------------------

小团队小草台，大团队大草台，老系统老屎山，新系统新屎山

---------------------------------------------------

语雀早就被钉钉团队合并了，其实原来内部是竞合关系，收编之后语雀的人走了很多

---------------------------------------------------

@julyclyde 我们其实是基于两种假设
我的假设是，人工操作不可靠，是人就一定会出问题，信任人的技能熟练程度不如信任工具的完善程度，因为工具的更新是一证永证的
你的假设是，问题持续在变化，能用工具覆盖的问题本身就不应该出现，所有出现的问题都是未知的，只能依赖工程师抢救
我觉得你的想法挺有道理的，就不争论这个了
回到问题本身，我认为语雀这个故障和处理方法，不管是哪种假设，它都不应该出现，因为完善的架构方案和工具链实在很多

---------------------------------------------------

@nekoharuya 首先，工具的更新并不是一证永证的
因为现实中的工具只是人的眼神，人有多大权限，工具最多也就有多大权限
语雀肯定管不了阿里云啊

假设我前面 44 层的猜测正确，那么阿里云提供抽象的服务，没有任何过错；
语雀的程序实际有本地状态却按无状态来处理，没有揭示风险给工具开发者，导致工具和被运维的目标不配套，但二者都是他们自己开发的，他们自己应该负全责。在这种情况下假设工具正确没有什么意义

甚至可能出现，刚开始二者是配套的，后来服务程序被谁改成了有本地状态，而工具依然按无状态来做，导致故障。这就是典型的违反你的一证永证假设的情况

---------------------------------------------------

多大的事儿，谷歌都全网挂过，各种分布式容灾，但还是挂了

---------------------------------------------------

玉伯都去字节了

---------------------------------------------------

@julyclyde 你 44 层的分析我认真看过了，这其实还是我最开始说的架构设计的问题，机器是物理机还是虚拟机，横竖都只是个容器，上层上个集群，下面的容器怎么更新都没关系，把机房拆了都行，语雀这种单例设计是我主要喷的点
然后你说的人的眼神，还有设计上无状态有状态的变化，这是不是很符合我说的，完全无法避免是个人一定会出错
既然不能避免，就更应该自动化，去依赖化
至于工具的一证永证当然指的是相同条件下，外部条件变了自然要重新开发，也就是我认同你说的依赖工程师解决全新的问题

---------------------------------------------------

抱歉，我个人觉得这个流程似乎没什么问题，时间上重建存储服务+日志恢复+完整性校验，作为只有单点冗余的结构
7 个小时应该是正常的。后面也说了会换两地三中心，除了删库这个时间点，暂时没看出有什么可喷的。

大家都是草台班子，谁也别笑谁。

---------------------------------------------------

@nekoharuya 你说的去依赖等等，都是“化”，是要去努力的目标
而不是现状

不能把这些作为现状去做假设，并在这种未经验证的假设之下去做其他事
更不能在此假设下自废武功

---------------------------------------------------

再大体量的平台，后面可能就几台单机跑着

---------------------------------------------------

裁员了

---------------------------------------------------

透露处理的细节内容太少了，

---------------------------------------------------

@dolphintwo 重建存储服务+日志恢复+完整性校验，这个流程，在故障已经发生时，是没有问题的
我主要分析的是，他的故障以及故障处理流程透露出来的背后的架构方案，开发和运维模式
上面已经讨论过很多楼，感兴趣可以自己康康

---------------------------------------------------

@aeli #20 内容没讲，只说运维工具有 bug ，一般来说操作资源的也就是 terraform 或者 pulumi 之类的声明式管理工具

---------------------------------------------------

@cherbim #1 下午做版本更新确实离谱，从这一点就可以看出这是个管理及其混乱的公司，以至于后面那些相互甩锅的行为就没那么不自然了

---------------------------------------------------

@julyclyde 这个不是努力目标啊……当然映射到个人可能是一种思想，但是在 2023 年，已经有很多成熟的框架和方案了，比如我说的分布式集群，结果他们就自己意识流开发，这是我说他野路子的根本原因啊……

---------------------------------------------------

@shakoon 为什么下午版本更新就草台班子了？如果一切正常滚动更新用户无感啊，谁没事跟你晚上更新啊

---------------------------------------------------

@julyclyde 顺带一提这里参考的体系是微软

---------------------------------------------------

别的不说，可以白嫖六个月的会员了

---------------------------------------------------

只有我一个人觉得上午、下午版本更新挺好的么？

上班时间干事，越早越好

谁愿意加班发版？晚高峰发版？

---------------------------------------------------

@nekoharuya #33  这是理论上，但现实里，预算不是无限的，机器的性能也不是无限的。更何况，公告里提到了，是某个 region 的节点故障，可没说只有一个节点故障，一个 raid5 挂两个不是一样凉凉？

---------------------------------------------------

@aper 周一下午更新确实是个奇怪的时间点，先参考敏捷体系，两周一个冲刺，冲刺完就更新，以周为单位，更新时间点当然不会是在周一，那 IPD 体系呢，IPD 体系纯瀑布流开发，所有设计在事先都规划好，所有的技术细节都通过假设和验证了，再进入开发，开发完了以后还有繁琐的测试，参考华为和甲骨文，所以 IPD 的概率也不大，不是主流体系，那就是意识流了，意识流开发，然后选择在周一下午这样的用户在线峰值时间段，做没有严格测试过的更新，所以是草台班子

---------------------------------------------------

@nekoharuya #21 你觉得不可思议就说不可思议啊，直接造谣算几个意思。

---------------------------------------------------

世界就是个草台班子，一群裱糊匠对付对付完事儿

---------------------------------------------------

@aper 不过这个问题不是那么重要，大家谁还不是个意识流了

---------------------------------------------------

就算数据库全坏了，也不至于网页都打不开，顶多有些操作报错而已。官网和存储用户笔记 是两回事吧。
正常来说，官网只是个前端页面。这个通告很假

---------------------------------------------------

随时随地可更新的才是正规军，

你以为语雀开发很多？

我认为开发加测试加产品 10 个有点多了。

---------------------------------------------------

@lambdaq 下午不下午不要紧，主要是周一，要么周末出去玩回来，忘了自己写啥了的情况下更新，要么早上摸摸鱼，下午直接更新，当然这是开玩笑，一般避开用户峰值，大多会选择周四什么的，不过这个不是很重要

---------------------------------------------------

@pkoukk 加机器数量的成本肯定是比怼单台机器性能低的，这也是分布式能够存在的根本原因，然后，可能我用词不够准确，一套 raid ，不管是 015610 ，都是算作“单例”的，因为都不是独立的多个环境

---------------------------------------------------

难道用户就没错吗？ 😊

---------------------------------------------------

@encro @encro 你这理论，微软听了沉默，谷歌看了流泪啊，张一鸣要是早点认识你，搞组织改革的钱都省了，大家把华为的书一扔，再也不搞什么 ipd 了

---------------------------------------------------

再严格的工具也也挡不住人的操作，前两年在和阿里差不多同级别的公司，碰到不小心把 K8S 集群（千台机器规模）的所有 master 给批量下线的误操作，喊来一堆大佬，搞了一晚上才恢复。啥容灾主从多 region 碰到这种都不好使。

---------------------------------------------------

b 站是不是也挂一次，b 站出的说明就比较详细

---------------------------------------------------

@NCZkevin 我想起之前公司，游戏内测第二天，老板打开谷歌云后台，自己瞎操作，把服务器删了……

---------------------------------------------------

@NCZkevin 后来公司寄了，这大哥现在在 supercell 上班，希望他别把人服务器也删了

---------------------------------------------------

@lambdaq 现在不都是半夜发么，加班之后又没有加班费又不让第二天休，硬尬

---------------------------------------------------

@nekoharuya 你开发发布还遵从啥敏捷体系？这玩意我自从大学书上学过后面就再没碰过了，你真是写代码的吗？

---------------------------------------------------

看样子 OP 仍然信任语雀
我觉得此文不是七小时后写的，而是写了七小时

---------------------------------------------------

@seeu2ex 个人觉得半夜发版这种做法很 toxic 。工作文化不怎么好。

@nekoharuya  语雀这种峰值好像就是上班时间。。。只能在下班时间发版？尴尬

---------------------------------------------------

“服务语雀的数据存储运维团队”，所以这数据库运维不是语雀的人，是蚂蚁金服干的？

---------------------------------------------------

@aper 碰啊，你翻翻我发的第一个帖子，如果能看懂，咱俩加个 v 聊聊 ai 撒，体系这种东西，你不碰，大概率就是别人代偿了，比如游戏公司一般是策划代偿了，我之前一路混到了独角兽公司的架构师岗位，现在在小创业公司做 CTO ，我管理水平也就半桶水，就之前被逼着学一点，主要还是靠写代码的硬实力

---------------------------------------------------

@weiwenhao 笑死，虽然内部肯定用的另一套，本地部署的

---------------------------------------------------

"就正常的 devops ，后台管理面板里，全自动维护，包括版本控制，回滚，备份，集群，镜像，机器冗余，全部自动化管理
这不该是现在的标配吗" 

把我干沉默了， 哪来的标配。能做好这一套的基本都是大厂，但语雀这体量算是大厂了。

---------------------------------------------------

@lambdaq 语雀上线也不是一天两天了，大家上班也不是每天 24 小时都用什么钉钉飞书，他们后台肯定是有在线数据分析的，我觉得大概是更新的人觉得改动很小，不重视，没当回事，而且又是和用户没有直接关系的运维工具，从我的经验看，包括我自己，大部分人都会有这种蜜汁，然后闹出事故的经历的……不过这就是我瞎猜的了

---------------------------------------------------

@nekoharuya 你在 21 楼分析的跟我想的差不多，你的帖子里面说“正常的架构思维里，所有的服务，就不应该跑在同一台机器上”。我猜测他们可能不是跑在同一台机器上，而是所有的机器都是祖传的，一升级全升级，然后集群炸了，反正我是不相信他们一台机器上存储这整个华东地区的数据

另外就是他们自己也说了是存储服务器，我的工作经历比较少，但是我觉得大部分公司的存储服务器应该不会跑在容器里吧，所以从这一点上，你说的“就正常的 devops ....”是不成立的，存储服务器炸了只能通过“硬件团队尝试将下线机器重新上线”这种方式

以上观点的都是建立在他们的公告为真的情况下分析

---------------------------------------------------

看来大多数人都是付费用户

---------------------------------------------------

谁能告诉我为什么语雀不用阿里云的 RDS

---------------------------------------------------

@nekoharuya 对。这次明显跟研发团队没关系。就是运维。。。

目测 yaml 写错了导致存储集群下线。

---------------------------------------------------

@VB1 容器这里是我用词不准确，这里的语境是，不管是虚拟机还是物理机都是容纳数据的容器
当然，产生歧义是我不对
然后，从真实场景上看，你看上面的回复，大多数人认为，他们就是上了套 raid ，具体哪套 raid 不清楚，不过不重要，没有更多的信息，但是应该大差不差
我的观点是，这种不是独立环境的，可以称为一套/一台/一个机器

---------------------------------------------------

@lambdaq 研发和运维一体，就叫 Devops ，你看，这个词在这个帖子里，应该是我用得最多的了

---------------------------------------------------

上个月我们公司部分服务不可用 10 分钟，人事要扣我 50%的绩效。语雀的运维估计得开掉一批，换一批更坑的替代。

---------------------------------------------------

再草包有人能纠错就不算草包，没人敢纠错的才是真草包。

---------------------------------------------------

> 导致华东地区生产环境存储服务器被误下线
> 升级硬件版本和机型，实现离线后的快速上线。该措施在本次故障修复中已完成

"下线" 这个词应该就是下线, 没有数据丢失. 不能再次上线正是因为自动化工具不支持, 才导致的. 自动化工具应该之前做过升级, 后续就只支持新机器了.

语雀做的就是找了新机器, 备份了老机器上的数据, 在新机器上重新部署了老机器上的数据服务.

我觉得问题核心应该是 "及时升级" (无论是软件依赖还是底层硬件).

---------------------------------------------------

@nekoharuya  s/devops/devoops/g

---------------------------------------------------

@iugo 不支持自动化工具，就采用老机器做数据迁移，是说不过去的。

---------------------------------------------------

@lambdaq #90 国内中小公司少见工作时间发版吧

---------------------------------------------------

@seeu2ex 现状的确如此。但是我讨厌这个现状。

---------------------------------------------------

@wangshushu #51 两家公司, 啥时蚂蚁把语雀卖给阿里云了

---------------------------------------------------

目前的现状可能是只要东西能跑,这就是一个稳定安全的服务,至于什么时候暴雷,相信下一任的智慧

---------------------------------------------------

修卡军团可还行 DOGE

---------------------------------------------------

@seeu2ex 我上家公司新来了个老大，说要做到上班时间发版， 出了一次生产事故后，发版时间反而比他来之前还延后了半小时...

---------------------------------------------------

阿里的东西，不赚钱就不重视，结果出了大纰漏。前面的会员也是吐槽得不行，连玉伯都走了。

---------------------------------------------------

是人就会犯错

---------------------------------------------------

再大的公司都会出纰漏的，gitlab 出现过线上库被删的问题。之前某 dns 提供商出错导致大量美国的网站没法访问。我觉得语雀这个算是比较轻的问题了

---------------------------------------------------

@cherbim 我建议别抨击这个，我以前待的一家公司开发的系统，每次都是晚上十点多开始升级，每一两周来一次，每次升级就是接近一个通宵。同为程序员，我觉得服务短期不可用不会死（做好了各种回滚策略，白天升级也不至于导致服务不可用），长此以往的熬夜升级服务是真的会猝死。

---------------------------------------------------

@hancai #112 https://i.imgur.com/n119Wvk.png 主要是加班一点福利都没，😑

---------------------------------------------------

他们没有运维团队和灾备系统😁

---------------------------------------------------

说阿里草台班子也大可不必，至少从概率上来说，阿里的员工技术、架构等经验应当会大于大部分公司。程序员老是相轻，与我见过的理发师的做法是一样的，经常理发的时候理发师都要抨击一下上一任给我剪头发的剪的烂，实则自己也是下一次别人抨击的对象。

---------------------------------------------------

我有个问题没想明白，存储服务下线了，为啥官网也挂了呢？有没有大佬能解答一下的

---------------------------------------------------

15:00 确认因存储系统使用的机器类别较老，无法直接操作上线

估计是阿里云上建不了旧规格的实例

---------------------------------------------------

@aper #68 这就是个风险控制的问题。举个可能你还不知道的例子，现在还在亚残运会期间，全国的国企都是禁止做系统变更的，处理紧急情况也要经过比平常的投产更高层级的审批。

---------------------------------------------------

这种等级的公司不会高峰时间发版的。这种多数是运维工具测试中把一些任务分配到了生产环境。。

---------------------------------------------------

感觉应该不止一台 DB 服务器被下线，而是整个 DB 所有实例都下线了，然后从备份进行了一次完整恢复

---------------------------------------------------

@codcrafts 数据没恢复前，不能对外呀，不然影响更恶劣

---------------------------------------------------

@Aliencn #16 不是说用了两个小时校验数据么，可能是通过日志将丢失的那部分恢复了

---------------------------------------------------

@shakoon 这个是特殊时期禁止发版，就像过年前一周禁止发版，完全不一样的事情。难道每天都 3 个小时都禁止发版，业务得吭哧吭哧半夜脑子一坨糊的时候跑来发版是吧

---------------------------------------------------

从公告的内容来看，“服务语雀的数据存储运维团队在进行升级操作”这句的意思给人的感觉是第三方（或者说不是语雀自己的团队）由此推断：
1 、语雀团队更关注功能和日常的运行维护，不涉及存储管理（或者说只是使用存储资源）
2 、存储资源可能是第三方资源（阿里云或是蚂蚁内部存储资源），应该是分布式存储（类似于 ceph ）
3 、存储集群的服务器资源使用的是旧服务器（新服务器可能是另一个群集），旧群集基于之前的业务可能长时间都未更新升级（能用），也没有考虑逐步迁移的新群集
4 、旧存储群集出现问题（例如硬件故障导致磁盘空间或是同步故障等），受技术限制（内部没有技术资源长期维护旧版本）短时间无法迁移切换到新群集
5 、通过数据恢复的方式迁移切换到新的环境（但愿）

---------------------------------------------------

草台班子理论

---------------------------------------------------

@nekoharuya #79 
软件工程没有银弹，分布式不能解决所有问题。
加机器只能在一定范围内有效，随着数据规模和机器规模的增长，你会发现有太多问题是不可拆分的，费劲把他们拆开送到子节点，再回收回来带来的一致性等等问题成本远高于分布式的收益。
目前最简单的例子就是跑 AI ，大模型的任务没办法切分，只能靠 NVLINK 这样的方式强行把多台机器并成一个超大的单机来用。

---------------------------------------------------

临时工

---------------------------------------------------

@fish267 问题是一开始官网是无法访问的，下午 4 点多的时候，我再次进官网是 502 ，说明这个时候负载均衡啥的已经过了

---------------------------------------------------

问题不大，毕竟这个世界都是一个草台班子

---------------------------------------------------

没准是裁员导致的管理问题。

---------------------------------------------------

@nekoharuya 运维工具应该在不在应用所在的服务器上。运维工具权限很大很奇怪么，弹性扩缩容，服务自动迁移都需要运维工具管理一批机器。大公司运维平台迭代较快的话，很多 n 年老服务下线就是很难起来了，需要重新适配新的启停脚本/重做镜像之类。

---------------------------------------------------

没必要咬文嚼字，这个公告就是给普通用户随便看看，真实的原因除了当事人谁会知道。

---------------------------------------------------

@pkoukk 拜托考虑下场景，语雀这样的在线文档/协作工具，面对的问题不是大模型那样的线性数据迭代，而是数据规模和用户并发压力
在这个背景下，把不同的数据表/块裂分到不同的主从服务器组里，各自承担对应部分的读写压力，再另起队列慢慢同步不属于自己负责区域的数据
单个主从服务器组炸了可以随时滚一个新的出来，也可以让其他组多承担一部分
这种简单有效低成本的设计，牺牲的只是一点磁盘空间，对于网络，cpu 性能，磁盘性能的要求，全部降低了不止一个数量级，单组机器只用抗住拆分后需要负责的数据规模，即使它炸了也能自动让其他组顶上
经典案例可以参考 telegram 的做法，每个用户的数据，都由不同的数据中心进行处理

---------------------------------------------------

@4kingRAS #9 不太可能是应用级的问题，应用级的问题回滚一下就好了，再差顶多几十分钟一个小时也就处理完成了。

---------------------------------------------------

@bugmakerxs 我来北京以后，去的第一家公司，只待了一个月就跑路的原因，就是看着祖传遗产害怕，老板跟我说我工位的机器，是网易现在的技术总监当年用过的……到我手里都不知道多少手了，一大堆和业务强关联的工具链，根本不敢乱动，交接文档都由不同的人写了好些份……

---------------------------------------------------

OP 大概是一路成长比较顺利，没见过这世界的背面
将来估计要吃大亏

---------------------------------------------------

@hancai 哈哈哈，同样，OP 应该是大厂的，所以这样认为。

---------------------------------------------------

前端写的系统，稳定性能好到哪里去

---------------------------------------------------

等下故意的呢？这波 IT 圈到处讨论，各种卖课公众号也是各种讲。
顺便分析一波你们对语雀的依赖情况·🤪

---------------------------------------------------

看看俄罗斯再看看以色列，世界的本质就是草台班子组成的啊

---------------------------------------------------

@Aliencn 当年有赞还说没有丢数据呢，事实可不是这个样子

---------------------------------------------------

想想当年 gitlab 更寄
https://www.bilibili.com/video/BV1uM4y1e7Mo/

---------------------------------------------------

歪个楼, 那我是不是可以白嫖六个月会员啊??

---------------------------------------------------

觉得可能是旧机器被删了，新机器盘符映射路径不对，导致数据损坏？

---------------------------------------------------

@hancai 哈哈哈，同样，OP 应该是大厂的，所以这样认为。

---------------------------------------------------

前端写的系统，稳定性能好到哪里去

---------------------------------------------------

等下故意的呢？这波 IT 圈到处讨论，各种卖课公众号也是各种讲。
顺便分析一波你们对语雀的依赖情况·🤪

---------------------------------------------------

看看俄罗斯再看看以色列，世界的本质就是草台班子组成的啊

---------------------------------------------------

@Aliencn 当年有赞还说没有丢数据呢，事实可不是这个样子

---------------------------------------------------

想想当年 gitlab 更寄
https://www.bilibili.com/video/BV1uM4y1e7Mo/

---------------------------------------------------

歪个楼, 那我是不是可以白嫖六个月会员啊??

---------------------------------------------------

觉得可能是旧机器被删了，新机器盘符映射路径不对，导致数据损坏？

---------------------------------------------------

@hancai 哈哈哈，同样，OP 应该是大厂的，所以这样认为。

---------------------------------------------------

前端写的系统，稳定性能好到哪里去

---------------------------------------------------

等下故意的呢？这波 IT 圈到处讨论，各种卖课公众号也是各种讲。
顺便分析一波你们对语雀的依赖情况·🤪

---------------------------------------------------

看看俄罗斯再看看以色列，世界的本质就是草台班子组成的啊

---------------------------------------------------

@Aliencn 当年有赞还说没有丢数据呢，事实可不是这个样子

---------------------------------------------------

想想当年 gitlab 更寄
https://www.bilibili.com/video/BV1uM4y1e7Mo/

---------------------------------------------------

歪个楼, 那我是不是可以白嫖六个月会员啊??

---------------------------------------------------

觉得可能是旧机器被删了，新机器盘符映射路径不对，导致数据损坏？

